{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CCSPNet.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Singular-Brain/CCSPNet/blob/main/CCSPNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLqHe2QUdFtb"
   },
   "source": [
    "#Preparing notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YfKf4DRMF2cN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a0bd5012-a0ae-48c4-bc19-a3564bbbd0d2"
   },
   "source": [
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy import io, signal, fftpack\n",
    "import scipy.linalg as la\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "###Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "import matplotlib\n",
    "\n",
    "from termcolor import colored\n",
    "import wget\n",
    "import mat73\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kgkQ4Lcj_Y64",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6529ca8d-7bc6-477f-9e7e-dc70f38d3da8"
   },
   "source": [
    "#set manual seed for preprocessing\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    from torch.backends import cudnn\n",
    "    cudnn.deterministic = True #type: ignore\n",
    "    cudnn.benchmark = False # type: ignore\n",
    "# We set the seed to 2045 because the Singularity is near!\n",
    "# manual_seed(2045)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    device = 'cuda'\n",
    "    workers = 2\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    workers = 0\n",
    "print(f'Device is set to {device}\\nNumber of workers: {workers}')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is set to cpu\n",
      "Number of workers: 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIGxkklzp7Zq"
   },
   "source": [
    "# Importing data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-EC1Ovav6rzT"
   },
   "source": [
    "class EEG_MI_dataset(Dataset):\n",
    "    def __init__(self, data, label, subject, mode):\n",
    "        if mode != 'Stability':\n",
    "            subject_idx  = int(subject) - 1\n",
    "            subject = f'{int(subject):02d}'\n",
    "        else:\n",
    "            subjs = copy.deepcopy(subject)\n",
    "            subjs.append(0)\n",
    "            subject = f'{int(0):02d}'\n",
    "            subject_idx=0\n",
    "        self.section = \"train\"\n",
    "        all_subjects = [str(format(i, '02d')) for i in range(1,55)]\n",
    "        if mode == 'SI':\n",
    "            ## Data\n",
    "\n",
    "            ## 5300\n",
    "            # self.data_tot = np.delete(data[:,3:,...], obj = subject_idx, axis = 0)\n",
    "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
    "            # self.labels_tot = np.delete(label[:,3:,...], obj = subject_idx, axis = 0)\n",
    "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
    "            # self.labels_tot = np.ravel(self.labels_tot)\n",
    "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
    "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
    "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
    "\n",
    "            ## 10600 Online\n",
    "            self.data_tot = np.delete(data[:,[1,3],...], obj = subject_idx, axis = 0)\n",
    "            self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
    "            self.labels_tot = np.delete(label[:,[1,3],...], obj = subject_idx, axis = 0)\n",
    "            self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
    "            self.labels_tot = np.ravel(self.labels_tot)\n",
    "            self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
    "            self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
    "            self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
    "            print(self.data_train.shape)\n",
    "\n",
    "            ## 10600 Offline\n",
    "            # self.data_tot = np.delete(data[:,[0,2],...], obj = subject_idx, axis = 0)\n",
    "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
    "            # self.labels_tot = np.delete(label[:,[0,2],...], obj = subject_idx, axis = 0)\n",
    "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
    "            # self.labels_tot = np.ravel(self.labels_tot)\n",
    "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
    "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
    "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
    "            # print(self.data_train.shape)\n",
    "\n",
    "            ## All Data\n",
    "            # self.data_tot = np.delete(data[:,:,...], obj = subject_idx, axis = 0)\n",
    "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
    "            # self.labels_tot = np.delete(label[:,:,...], obj = subject_idx, axis = 0)\n",
    "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
    "            # self.labels_tot = np.ravel(self.labels_tot)\n",
    "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
    "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
    "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
    "            # print(self.data_train.shape)\n",
    "\n",
    "        elif mode == 'SD':\n",
    "            self.val_idx = np.random.randint(0,200,40)\n",
    "            ## Data\n",
    "            self.data_tot = data[subject_idx,[0,1,2],...]\n",
    "            self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[2:])\n",
    "            self.label_tot = label[subject_idx,[0,1,2],...]\n",
    "            self.label_tot = self.label_tot.reshape((-1,1) +  self.label_tot.shape[2:])\n",
    "            self.label_tot = np.ravel(self.label_tot)\n",
    "\n",
    "            self.data_train, self.data_val, self.labels_train, self.labels_val = train_test_split(self.data_tot, self.label_tot, test_size=0.2, random_state=2045, shuffle=True)\n",
    "            self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
    "            self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
    "\n",
    "        ## For robustness tests\n",
    "        elif mode == 'Stability':\n",
    "            ## Data\n",
    "\n",
    "            ## 5300\n",
    "            # self.data_tot = np.delete(data[:,3:,...], obj = subject_idx, axis = 0)\n",
    "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
    "            # self.labels_tot = np.delete(label[:,3:,...], obj = subject_idx, axis = 0)\n",
    "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
    "            # self.labels_tot = np.ravel(self.labels_tot)\n",
    "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
    "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
    "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
    "\n",
    "            ## 10600 Online\n",
    "            # data = data[subjs,...]\n",
    "            # self.data_tot = np.delete(data[:,[1,3],...], obj = 0, axis = 0)\n",
    "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
    "            # label = label[subjs,...]\n",
    "            # self.labels_tot = np.delete(label[:,[1,3],...], obj = 0, axis = 0)\n",
    "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
    "            # self.labels_tot = np.ravel(self.labels_tot)\n",
    "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
    "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
    "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
    "            # print(self.data_train.shape)\n",
    "\n",
    "            ## 10600 Offline\n",
    "            data = data[subjs,...]\n",
    "            self.data_tot = np.delete(data[:,[0,2],...], obj = subject_idx, axis = 0)\n",
    "            self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
    "            label = label[subjs,...]\n",
    "            self.labels_tot = np.delete(label[:,[0,2],...], obj = subject_idx, axis = 0)\n",
    "            self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
    "            self.labels_tot = np.ravel(self.labels_tot)\n",
    "            self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
    "            self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
    "            self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
    "            print(self.data_train.shape)\n",
    "\n",
    "            ## All Data\n",
    "            # self.data_tot = np.delete(data[:,:,...], obj = subject_idx, axis = 0)\n",
    "            # self.data_tot = self.data_tot.reshape((-1,1) +  self.data_tot.shape[3:])\n",
    "            # self.labels_tot = np.delete(label[:,:,...], obj = subject_idx, axis = 0)\n",
    "            # self.labels_tot = self.labels_tot.reshape((-1,1) +  self.labels_tot.shape[3:])\n",
    "            # self.labels_tot = np.ravel(self.labels_tot)\n",
    "            # self.data_train,  self.labels_train = self.data_tot, self.labels_tot\n",
    "            # self.data_test = data[subject_idx, 3,:,np.newaxis,...]\n",
    "            # self.labels_test = np.ravel(label[subject_idx, 3,:])\n",
    "            # print(self.data_train.shape)        \n",
    "               \n",
    "        ### Emptying Ram\n",
    "        for element in [data, label]:\n",
    "            del(element)\n",
    "        ### Copying arrays\n",
    "        self.data_train, self.labels_train  = self.data_train.copy(), self.labels_train.copy()\n",
    "        self.data_test,  self.labels_test   = self.data_test.copy(),  self.labels_test.copy()\n",
    "\n",
    "    def train(self):\n",
    "        self.section = \"train\"\n",
    "\n",
    "    def val(self):\n",
    "        self.section = \"val\"\n",
    "\n",
    "    def test(self):\n",
    "        self.section = \"test\"\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.section == 'test':\n",
    "            return len(self.labels_test)\n",
    "        elif self.section == 'val':\n",
    "            return len(self.labels_val)\n",
    "        elif self.section == 'train':\n",
    "            return len(self.labels_train)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.section == 'test':\n",
    "            return self.data_test[idx], self.labels_test[idx]\n",
    "        elif self.section == 'val':\n",
    "            return self.data_val[idx], self.labels_val[idx]\n",
    "        elif self.section == 'train':\n",
    "            return self.data_train[idx], self.labels_train[idx]\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1EdGcoJfAYR"
   },
   "source": [
    "\n",
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIuHkoHWKJKH"
   },
   "source": [
    "\n",
    "\n",
    "Link to the preprocessed data and label:\n",
    "\n",
    "https://drive.google.com/drive/folders/1pcskugvKgo5sCuFzAJbiK6xNO1up12JO?usp=sharing\n",
    "\n",
    "Create a shortcut of the shared folder in your Drive, and run the following cell to load the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SYjLE_FMa1Nj"
   },
   "source": [
    "data    =  mat73.loadmat('/Users/almacuevas/work_projects/voting_system_platform/Datasets/CCSPNet_dataset/Preprocessed_Data.mat')['Data']\n",
    "data    =  np.moveaxis(data , -1, -2)\n",
    "labels  =  io.loadmat('/Users/almacuevas/work_projects/voting_system_platform/Datasets/CCSPNet_dataset/Labels.mat')['Labels']"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu65V7GqH_ML"
   },
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Dj6NTijsipmy"
   },
   "source": [
    "class CCSP(nn.Module):\n",
    "    def __init__(self, kernLength, timepoints, nb_classes = 2 ,wavelet_filters = 4, wavelet_kernel = 64 ,nn_layers= [4],feature_reduction_network = [4], nchans = 62, n_CSP_subspace_vectors = 1 ,):\n",
    "        super(CCSP, self).__init__()\n",
    "        #manual_seed(2045)\n",
    "        self.T = timepoints     \n",
    "        self.m = n_CSP_subspace_vectors\n",
    "        self.F = nn_layers[-1] if nn_layers else wavelet_filters\n",
    "        self.CSP = CommonSpatialPattern(self.F, self.m, nchans)\n",
    "        self.register_parameter(name='CSP projection matrix', param= self.CSP.W)\n",
    "        self.CSP_fig = None\n",
    "        self.kernLength = kernLength\n",
    "        self.wavelet_filters = wavelet_filters\n",
    "        self.wavelet_kernel = wavelet_kernel\n",
    "        self.nn_layers = nn_layers\n",
    "        self.batch_norm = nn.BatchNorm1d(2* self.m * self.F)\n",
    "        self.feature_reduction_network = feature_reduction_network\n",
    "        ### Wawelet\n",
    "        self.wavelet_padding = nn.ZeroPad2d((int(wavelet_kernel/2)-1, int(wavelet_kernel/2), 0, 0))\n",
    "        self.freq = torch.tensor([[4 + (i+1) * 36/(self.wavelet_filters + 1)] for i in range(self.wavelet_filters)], requires_grad= True, device= device)\n",
    "        self.fwhm = torch.tensor([[.1] for _ in range(self.wavelet_filters)], requires_grad= True, device= device)\n",
    "        self.coefficient = torch.tensor([[4* math.log(2)] for _ in range(self.wavelet_filters)], requires_grad= True, device= device)\n",
    "        ### Neural Network\n",
    "        self.CNN = nn.ModuleList()\n",
    "        prev_chans = wavelet_filters if wavelet_filters else 1\n",
    "        for i, chans in enumerate(nn_layers):\n",
    "            if i == len(nn_layers) -1:\n",
    "                last_block= True\n",
    "            else:\n",
    "                last_block=False\n",
    "            self.CNN.append(self.CNN_block(prev_chans, chans, last_block))\n",
    "            prev_chans = chans\n",
    "        ### FRN\n",
    "        self.FRN = nn.ModuleList()\n",
    "        prev_size = self.F * 2 * self.m\n",
    "        for i, size in enumerate(feature_reduction_network):\n",
    "            if i == len(feature_reduction_network) -1:\n",
    "                last_block= True\n",
    "            else:\n",
    "                last_block=False\n",
    "            self.FRN.append(self.FRN_block(prev_size, size, last_block)) \n",
    "            prev_size = size\n",
    "        ### LDA\n",
    "        if self.FRN:\n",
    "            self.LDA = LinearDiscriminantAnalysis(feature_reduction_network[-1])\n",
    "        else:\n",
    "            self.LDA = LinearDiscriminantAnalysis(2 * self.F * self.m)\n",
    "        self.register_parameter(name='LDA projection matrix', param= self.LDA.W)\n",
    "\n",
    "    def plot_CSP(self, CSP_output, labels):\n",
    "        print('CSP Plots:')\n",
    "        label = labels.cpu()\n",
    "        rows = math.ceil(self.F/4)\n",
    "        self.CSP_fig = plt.figure(figsize = (16,4 * rows))\n",
    "        for i in range(self.F):\n",
    "            points = CSP_output[:,i,...].squeeze(-1).cpu().detach().numpy()\n",
    "            class0 = points[label==0]\n",
    "            class1 = points[label==1]\n",
    "            ax = plt.subplot(rows, 4, i+1)\n",
    "            ax.plot(class0[:,0], class0[:,-1], 'ob')\n",
    "            ax.plot(class1[:,0], class1[:,-1], 'or', alpha = .6)\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_xticklabels([])\n",
    "        plt.show()\n",
    "\n",
    "    def _design_wavelet(self, kernLength, freq, fwhm, coefficient, fs = 100):\n",
    "            timevec = torch.arange(kernLength) / fs\n",
    "            timevec = timevec - torch.mean(timevec)\n",
    "            timevec = timevec.repeat(self.wavelet_filters).reshape(self.wavelet_filters, kernLength).to(device)\n",
    "            csw = torch.cos(2*math.pi*freq*timevec)\n",
    "            gus = torch.exp(-(coefficient * torch.pow(timevec, 2)/torch.pow(fwhm,2)))\n",
    "            return (csw * gus).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "    def CNN_block(self, prev_chans, chan, last_block):\n",
    "        if not last_block:\n",
    "            return nn.Sequential(\n",
    "                nn.ZeroPad2d((int(self.kernLength/2)-1, int(self.kernLength/2), 0, 0)),\n",
    "                nn.Conv2d(prev_chans, chan, (1, self.kernLength), padding =0, bias = False),\n",
    "                nn.Sigmoid(),\n",
    "                nn.BatchNorm2d(chan),\n",
    "                )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ZeroPad2d((int(self.kernLength/2)-1, int(self.kernLength/2), 0, 0)),\n",
    "                nn.Conv2d(prev_chans, chan, (1, self.kernLength), padding =0, bias = False),\n",
    "                )\n",
    "\n",
    "    \n",
    "    def FRN_block(self, prev_size, size, last_block):\n",
    "        if not last_block:\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(prev_size, size),\n",
    "                nn.Sigmoid(),\n",
    "                nn.BatchNorm1d(size),\n",
    "                )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(prev_size, size),\n",
    "            )\n",
    "\n",
    "    def copy(self):\n",
    "        state = {}\n",
    "        state['wavelet'] = (copy.deepcopy(self.freq),\n",
    "                            copy.deepcopy(self.fwhm), \n",
    "                            copy.deepcopy(self.coefficient))\n",
    "        state['state dict'] = copy.deepcopy(self.state_dict())\n",
    "        state['CSP'] = copy.deepcopy((self.CSP.W.data).clone().detach().tolist())\n",
    "        state['LDA'] = copy.deepcopy((self.LDA.W.data).clone().detach().tolist())\n",
    "        return state\n",
    "\n",
    "    def paste(self, state):\n",
    "        self.freq, self.fwhm, self.coefficient = state['wavelet']\n",
    "        self.load_state_dict(state['state dict'])\n",
    "        self.CSP.W.data = torch.tensor(state['CSP'], device = device) \n",
    "        self.LDA.W.data = torch.tensor(state['LDA'], device = device)\n",
    "\n",
    "    def fit_modules(self, X, y):\n",
    "        _, data = self(X)\n",
    "        self.CSP.fit(data['CNN output'], y)\n",
    "        self.LDA.fit(data['LDA input'], y)\n",
    "\n",
    "    def pred(self, X, y):\n",
    "        training_state = self.training\n",
    "        self.training = False\n",
    "        Y, _ = self(X)\n",
    "        MU = torch.tensor([torch.mean(Y[y == 0]),torch.mean(Y[y==1])]).reshape(-1,1).to(device)\n",
    "        yhat = torch.zeros(X.shape[0])\n",
    "        for i, xi in enumerate(Y):\n",
    "            dis0 = torch.sqrt(torch.dot(((xi-MU[0]).T),(xi-MU[0])))\n",
    "            dis1 = torch.sqrt(torch.dot(((xi-MU[1]).T),(xi-MU[1])))\n",
    "            if dis0 <= dis1:\n",
    "                yhat[i] = 0\n",
    "            else:\n",
    "                yhat[i] = 1\n",
    "        self.training = training_state\n",
    "        return yhat.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x, visualization = False):\n",
    "        \"\"\"\n",
    "        x:   input - shape: [N, 1, NEc, Tp]\n",
    "        N:   Batch size\n",
    "        NEc: Number of EEG channels\n",
    "        Tp:  Time point\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        ### Wavelet:\n",
    "        if self.wavelet_filters:\n",
    "            x = self.wavelet_padding(x)\n",
    "            self.wavelet_weight = self._design_wavelet(self.wavelet_kernel, self.freq, self.fwhm, self.coefficient ) \n",
    "            x = F.conv2d(x, weight = self.wavelet_weight,bias = None)   # [N, F1, NEc, Tp]\n",
    "        ### Network:\n",
    "        for layer in self.CNN:\n",
    "            x = layer(x)                        \n",
    "        CNN_output =  x                                                 # [N, F1, NEc, Tp]\n",
    "        data['CNN output'] = CNN_output\n",
    "        ### CSP\n",
    "        Y = self.CSP(CNN_output)                                        # [N, F1, 2m, Tp]\n",
    "        CSP_output = torch.log(torch.var(Y, axis = -1))                 # [N, F1, 2m]\n",
    "        if visualization:\n",
    "            self.plot_CSP(CSP_output)\n",
    "        data[\"CSP output\"]= CSP_output\n",
    "        LDA_input = CSP_output.reshape((-1, 2 * self.m * self.F))       # [N, F1*2m]\n",
    "        LDA_input = self.batch_norm(LDA_input)\n",
    "        ### Feature Reduction Network\n",
    "        if self.FRN:\n",
    "            x = LDA_input\n",
    "            for layer in self.FRN:\n",
    "                x = layer(x)\n",
    "            LDA_input = x                                               # [N, last NN layer size]\n",
    "        ### LDA\n",
    "        output = self.LDA(LDA_input)                                    # [N, 1]\n",
    "        data['LDA input'] = LDA_input\n",
    "        return output, data           "
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aa7fSRudQXTR"
   },
   "source": [
    "##CSP & LDA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r36llDdI2cjl"
   },
   "source": [
    "def cov(m, y=None):\n",
    "    if y is not None:\n",
    "        m = torch.cat((m, y), dim=0)\n",
    "    m_exp = torch.mean(m, dim=1)\n",
    "    x = m - m_exp[:, None]\n",
    "    cov = 1 / (x.size(1) - 1) * x@(x.T)\n",
    "    return cov\n",
    "\n",
    "#### CSP\n",
    "class CommonSpatialPattern(nn.Module):\n",
    "    def __init__(self, n_temporal_filters, n_subspace_vectors, NEc):\n",
    "        super().__init__()\n",
    "        self.F = n_temporal_filters\n",
    "        self.m = n_subspace_vectors\n",
    "        self.W = Parameter(torch.ones((self.F ,2* self.m, NEc), device = device))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.W @ X\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if type(y) != np.ndarray:\n",
    "            y = y.cpu().detach().numpy()\n",
    "        unique_values = np.unique(y, axis = 0)\n",
    "        class0 = X[[True if (i==unique_values[0]).all() else False for i in y]] # (N_class 0 , F, NEc, Tp)\n",
    "        class1 = X[[True if (i==unique_values[1]).all() else False for i in y]] # (N_class 1 , F, NEc, Tp)\n",
    "        total_W =  torch.tensor([])                                             # (F, 2*m , NEc)\n",
    "        for i in range(self.F):\n",
    "            class0_i = class0[:,i,...]                                          # (N_class 1 , 1, NEc, Tp)\n",
    "            class1_i = class1[:,i,...]\n",
    "            class0_i = class0_i - class0_i.mean(axis = 2).reshape(class0_i.shape[0], class0_i.shape[1],1)\n",
    "            class1_i = class1_i - class1_i.mean(axis = 2).reshape(class1_i.shape[0], class1_i.shape[1],1)\n",
    "            RH = 0\n",
    "            for x in class1_i:\n",
    "                RH += ((x@x.T)/(torch.trace(x@x.T)+ 1E-6))\n",
    "            RH = RH / class1_i.shape[0]\n",
    "            ####\n",
    "            RL = 0\n",
    "            for x in class0_i:\n",
    "                RL += (x@x.T)/(torch.trace(x@x.T+ 1E-6))\n",
    "            RL = RL / class0_i.shape[0] \n",
    "            RL = RL.cpu().detach().numpy()\n",
    "            RH = RH.cpu().detach().numpy()\n",
    "            try:\n",
    "                v, u = la.eigh(RL, RH)\n",
    "            except Exception as e:\n",
    "                print('!', e)\n",
    "                RH += np.random.random(RH.shape) * 1E-4\n",
    "                RL += np.random.random(RL.shape) * 1E-4\n",
    "                v, u = la.eigh(RL, RH)\n",
    "            sorted_u = u[:,abs(v).argsort()[::-1]]\n",
    "            W = torch.tensor(np.concatenate((sorted_u[:,:self.m], sorted_u[:,-self.m:]), axis = 1)) #(NEc, 2m)\n",
    "            total_W = torch.cat((total_W, W.T.unsqueeze(0)), 0)                                     #(F, 2m, NEc)\n",
    "        self.W.data = total_W.to(device)                                                            #(F, 2m, NEc)\n",
    "\n",
    "\n",
    "###LDA        \n",
    "class LinearDiscriminantAnalysis(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.W = Parameter(torch.zeros((input_dim, 1), device = device))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return (self.W.T @ X.T).T\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if type(y) != np.ndarray:\n",
    "            y = y.cpu().detach().numpy()\n",
    "        unique_values = np.unique(y, axis = 0)\n",
    "        Xc1 = (X[[True if (i==unique_values[0]).all() else False for i in y]]).T # (N_class 0 , F, NEc, Tp)\n",
    "        Xc2 = (X[[True if (i==unique_values[1]).all() else False for i in y]]).T # (N_class 1 , F, NEc, Tp)\n",
    "\n",
    "        mu1 = Xc1.mean(axis=1).reshape(-1, 1)\n",
    "        mu2 = Xc2.mean(axis=1).reshape(-1, 1)\n",
    "        Sp1 = cov(Xc1)       #2m,  2m\n",
    "        Sp2 = cov(Xc2)\n",
    "        Sb = (mu1- mu2)@((mu1- mu2).T)\n",
    "        Sw = Sp1 + Sp2\n",
    "        Sw = Sw.cpu().detach().numpy()\n",
    "        Sb = Sb.cpu().detach().numpy()\n",
    "        try:\n",
    "            A = np.linalg.inv(Sw)@Sb\n",
    "        except Exception as e:\n",
    "            print('!', e)\n",
    "            Sw += np.random.random(Sw.shape) * 1E-4\n",
    "            A = np.linalg.inv(Sw)@Sb\n",
    "        u, v = la.eig(A)        #v: eigenvector , u: eigenvalue\n",
    "        sorted_v = v[:,np.argsort(abs(u))[::-1]]\n",
    "        ### Update W\n",
    "        self.W.data = torch.tensor(sorted_v[:, 0].reshape(-1, 1)).float().to(device) \n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pGLzTR3Tqux"
   },
   "source": [
    "##Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L0KHEWl_SvDG"
   },
   "source": [
    "def color(x):\n",
    "    if x < 60:\n",
    "        c = 'red'\n",
    "    elif x < 75:\n",
    "        c = 'yellow'\n",
    "    else:\n",
    "        c = 'green'\n",
    "    return colored(x, c)\n",
    "    \n",
    "from scipy.fft import rfft, rfftfreq\n",
    "def plot_fft(signal):\n",
    "    yf = rfft(signal)\n",
    "    xf = rfftfreq(64, 1 / 100)\n",
    "    plt.plot(xf, np.abs(yf))\n",
    "\n",
    "\n",
    "### Loss\n",
    "def LDALoss(pred, label, alpha = 0.001, epsilon = 1E-5):\n",
    "    def generator(X):\n",
    "        return 1/(X+1) - 1/2\n",
    "    pred = pred.reshape(-1).float()\n",
    "    label = label.reshape(-1).float()\n",
    "    return (label @ (pred) + (label-1) @ (pred)) / len(label)\n",
    "def MVLoss(pred, label):\n",
    "    epsilon = 1e-5\n",
    "    c0 = pred[label==0]\n",
    "    c1 = pred[label==1]\n",
    "    return (torch.var(c0) + torch.var(c1)) / ((c0.mean() - c1.mean())**2 + epsilon)\n",
    "\n",
    "def output_format(labels, m):\n",
    "    label_form = lambda label, m: [1- label] * m + [label] * m\n",
    "    return torch.tensor([label_form(label, m) for label in labels]).float().to(device)\n"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QSIY8SpTub6"
   },
   "source": [
    "#Train and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ftYKgprHTIRZ"
   },
   "source": [
    "def train(model, dataloader, epochs, m, validation = False, loss_ratio = .5,\n",
    "          lr = 0.01, wavelet_lr = 0.1, criterion = MVLoss, lambda1 = 1e-2, lambda2=1e-1,\n",
    "          early_stop_patience = None, verbose = 2, tensorboard = False,\n",
    "          scheduler = None):\n",
    "    assert verbose in [0,1,2], \"'vebose' can be one of 0, 1 or 2 \"\n",
    "    assert 0 <= loss_ratio <= 1, \"'loss_ratio must be between 0 and 1\"\n",
    "    manual_seed(2045)\n",
    "    optimizer = optim.Adam(model.parameters(), lr =lr, weight_decay=lambda2)\n",
    "    optimizer.add_param_group({'params': model.freq, \"lr\" : wavelet_lr * 10})\n",
    "    optimizer.add_param_group({'params': model.fwhm, \"lr\" : wavelet_lr})\n",
    "    optimizer.add_param_group({'params': model.coefficient, \"lr\" : wavelet_lr})\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2,\n",
    "    #        patience=5, min_lr=1E-5, verbose=True if verbose in [2] else False)\n",
    "    batch_size = dataloader.batch_size\n",
    "    n_batches = math.ceil(dataloader.dataset.data_train.shape[0]/batch_size)\n",
    "    if verbose in [1,2]:\n",
    "        print('Number of Trials:',dataloader.dataset.data_train.shape[0],\n",
    "              'Batch Size:', batch_size,'Number of Batches:', n_batches)\n",
    "        print('Train input shape:', dataloader.dataset.data_train.shape)\n",
    "    if tensorboard:\n",
    "        tb = SummaryWriter(f'runs/model01/{int(datetime.now().timestamp())}')\n",
    "    train_history = {\"train_loss\":[],\"train_accuracy\":[], \"model\": []}\n",
    "    training_start = time.time()\n",
    "    dataset.train()\n",
    "    all_inputs, all_labels = torch.tensor(dataloader.dataset.data_train).float().to(device), torch.tensor(dataloader.dataset.labels_train).to(device)\n",
    "    model.fit_modules(all_inputs, all_labels)\n",
    "    for epoch in range(epochs):\n",
    "        if verbose in [2, 3]:\n",
    "            print('='*70 + '\\n' + f'Epoch: {epoch + 1}/ {epochs}', end = '\\r' )\n",
    "        epoch_start = time.time()\n",
    "        running_loss = 0.0\n",
    "        if verbose in [2, 3]:\n",
    "            print(f'\\n@ Batch: {1} / {n_batches}', end = '\\r')\n",
    "        dataset.train()\n",
    "        for batch ,(inputs, labels) in enumerate(dataloader):\n",
    "            batch_start = time.time()\n",
    "            ### Wrap them in Variable\n",
    "            inputs, labels = Variable(inputs).float().to(device), Variable(labels).to(device)\n",
    "            ### Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            ###  Forward\n",
    "            model.train()\n",
    "            outputs, data = model(inputs)\n",
    "            if False:\n",
    "                rows = math.ceil(model.wavelet_filters/4)\n",
    "                plt.figure(figsize = (16,4 * rows))\n",
    "                for i in range(model.wavelet_filters):\n",
    "                    ax = plt.subplot(rows, 4, i+1)\n",
    "                    plt.plot(model.wavelet_weight[i,0,0,:].detach().cpu())\n",
    "                    ax.set_yticklabels([])\n",
    "                    ax.set_xticklabels([])\n",
    "                plt.show()\n",
    "                plt.figure(figsize = (16,4 * rows))\n",
    "                for i in range(model.wavelet_filters):\n",
    "                    ax = plt.subplot(rows, 4, i+1)\n",
    "                    plot_fft(model.wavelet_weight[i,0,0,:].detach().cpu().numpy())\n",
    "                plt.show()\n",
    "            ### CSP Loss\n",
    "            CSP_output_softmax = F.softmax(data['CSP output'], dim = -1)\n",
    "            CSP_loss = 0\n",
    "            BCElabels = output_format(labels, m)\n",
    "            for f in range(CSP_output_softmax.size(1)):\n",
    "                CSP_loss += nn.BCELoss()(CSP_output_softmax[:,f,...].float(), BCElabels)\n",
    "            CSP_loss = CSP_loss / CSP_output_softmax.size(1)\n",
    "            ### L1 reguralization\n",
    "            all_linear1_params = torch.cat([param.view(-1) for name, param in model.named_parameters() if 'weight' in name])\n",
    "            l1_regularization_loss = lambda1 * torch.norm(all_linear1_params, 1)\n",
    "            ### Backward + Optimize\n",
    "            #formatted_labels = output_format(labels, outputs)\n",
    "            loss = loss_ratio * CSP_loss + (1- loss_ratio) * criterion(outputs, labels.type(torch.LongTensor).to(device)) + l1_regularization_loss\n",
    "            loss.backward(retain_graph=False)\n",
    "            optimizer.step()\n",
    "            ### Calculate batch loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            predicted_labels = model.pred(inputs, labels)\n",
    "            batch_accuracy = (predicted_labels == labels).sum().item()/(len(inputs))\n",
    "            batch_end = time.time()\n",
    "            if verbose in [2, 3]:\n",
    "                print(f'\\033[K\\r@ Batch: {batch + 1} / {n_batches} | Batch Loss: { loss.item():.4f}  |  Batch Accuracy: {batch_accuracy * 100:.2f} | Batch Duration: {time.strftime(\"%H:%M:%S\", time.gmtime(batch_end - batch_start))}', end = '')\n",
    "        if verbose in [2, 3]:\n",
    "            print('\\nCalculating Accuracies...', end = '\\r')\n",
    "        ### Validation\n",
    "        with torch.no_grad():\n",
    "            ################ Update LDA & CSP\n",
    "            if batch_size < len(dataloader.dataset.data_train):\n",
    "                if verbose in [3]:\n",
    "                    ### Evaluate model loss and accuracy before updateing CSP & LDA\n",
    "                    model.eval()\n",
    "                    ts_corrects = 0\n",
    "                    dataset.val()\n",
    "                    for batch ,(test_inputs, test_labels) in enumerate(dataloader):\n",
    "                        test_inputs, test_labels = Variable(test_inputs).float().to(device), Variable(test_labels).to(device)\n",
    "                        test_outputs, test_data =  model(test_inputs)\n",
    "                        CSP_output_softmax = F.softmax(test_data['CSP output'], dim = -1)\n",
    "                        CSP_loss = 0\n",
    "                        BCElabels = output_format(test_labels, m)\n",
    "                        for f in range(CSP_output_softmax.size(1)):\n",
    "                            CSP_loss += nn.BCELoss()(CSP_output_softmax[:,f,...].float(), BCElabels)\n",
    "                        CSP_loss = CSP_loss / CSP_output_softmax.size(1)\n",
    "                        #formatted_labels = output_format(test_labels, test_outputs)\n",
    "                        test_loss = loss_ratio * CSP_loss + (1 - loss_ratio) * criterion(test_outputs, labels.type(torch.LongTensor).to(device))\n",
    "                        predicted_labels = model.pred(test_inputs, test_labels).to(device)\n",
    "                        ts_corrects += (predicted_labels == test_labels).sum().item()\n",
    "                        ts_acc =  (predicted_labels == test_labels).sum().item()/(len(test_inputs))\n",
    "                        print(f'\\033[Kaccuracy before updating CSP and LDA | @ Batch: {batch + 1} / | Acc = {ts_acc}', end = '\\r')\n",
    "                    total_ts_acc = (ts_corrects/len(dataloader.dataset))\n",
    "                    print(f'\\033[Kaccuracy before updating CSP and LDA: {total_ts_acc:.4f}                                ')\n",
    "                    \n",
    "                ### forward pass: Update the projection matrixes of CSP and LDA\n",
    "                if verbose in [2, 3]:\n",
    "                    print('\\033[KForward pass', end = '\\r')\n",
    "                model.fit_modules(all_inputs, all_labels)\n",
    "                ### Calculate new train loss and accuracy\n",
    "                if verbose in [2, 3]:\n",
    "                    print('\\rCalculating new Train Loss and Accuracy', end = '\\r')\n",
    "                predicted_labels = model.pred(inputs, labels)\n",
    "                total_tr_acc = (predicted_labels == labels).sum().item()/(len(inputs))\n",
    "                total_tr_loss = loss\n",
    "            else:\n",
    "                total_tr_acc = batch_accuracy\n",
    "                total_tr_loss = loss\n",
    "\n",
    "            ######################\n",
    "            ### Calculate test Loss & Accuracy\n",
    "            if validation:\n",
    "                if verbose in [2]:\n",
    "                    print('\\rCalculating test Loss & Accuracy', end = '\\r')\n",
    "                model.eval()\n",
    "                ts_corrects = 0\n",
    "                dataset.val()\n",
    "                for batch ,(test_inputs, test_labels) in enumerate(dataloader):\n",
    "                    test_inputs, test_labels = Variable(test_inputs).float().to(device), Variable(test_labels).to(device)\n",
    "                    test_outputs, test_data =  model(test_inputs)\n",
    "                    CSP_output_softmax = F.softmax(test_data['CSP output'], dim = -1)\n",
    "                    CSP_loss = 0\n",
    "                    BCElabels = output_format(test_labels, m)\n",
    "                    for f in range(CSP_output_softmax.size(1)):\n",
    "                        CSP_loss += nn.BCELoss()(CSP_output_softmax[:,f,...].float(), BCElabels)\n",
    "                        # print('*', f, ':',BCE.sum(axis = -1).mean().item())\n",
    "                    CSP_loss = CSP_loss / CSP_output_softmax.size(1)  \n",
    "                    test_loss = loss_ratio * CSP_loss + (1 - loss_ratio) * criterion(test_outputs, test_labels.type(torch.LongTensor).to(device))\n",
    "                    # print('TEST: LDA Loss', criterion(test_outputs, test_labels) ,' , CSP Loss', CSP_loss)\n",
    "                    predicted_labels = model.pred(test_inputs, test_labels).to(device)\n",
    "                    ts_corrects += (predicted_labels == test_labels).sum().item()\n",
    "                    ts_acc =  (predicted_labels == test_labels).sum().item()/(len(test_inputs))\n",
    "                    if verbose in [2]:\n",
    "                        print(f'\\033[KNew Test Accuracy | @ Batch: {batch+1} / | Acc = {ts_acc}', end = '\\r')\n",
    "                total_ts_acc = (ts_corrects/len(dataloader.dataset))\n",
    "        ### TensorBoard\n",
    "        if tensorboard:\n",
    "            if verbose in [2, 3]:\n",
    "                print('\\033[KWrite on Tensorboard...', end = '\\r')\n",
    "            for name, w in model.named_parameters():\n",
    "              if 'conv' in name:\n",
    "                tb.add_histogram(name, w, i)\n",
    "                tb.add_histogram(str(name) + ' grad', w, i)\n",
    "        ### Results\n",
    "        train_history[\"train_loss\"].append(running_loss/n_batches)\n",
    "        train_history[\"train_accuracy\"].append(total_tr_acc)\n",
    "        if validation:\n",
    "            train_history[\"val_loss\"].append(test_loss.item())\n",
    "            train_history[\"val_accuracy\"].append(total_ts_acc)\n",
    "        train_history[\"model\"].append(model.copy())\n",
    "        if early_stop_patience:\n",
    "            if epoch > early_stop_patience:\n",
    "                if train_history[\"val_loss\"][- early_stop_patience] < min( train_history[\"val_loss\"][1 - early_stop_patience:]):\n",
    "                    if verbose in [2, 3]:\n",
    "                        print(\"Early stop!\")\n",
    "                    break\n",
    "        if scheduler:\n",
    "            scheduler.step(running_loss)\n",
    "        epoch_end = time.time()\n",
    "        if verbose in [2, 3]:\n",
    "            print(f\"\\033[K ► Training Loss=  {running_loss/n_batches:.4f} |  Train Acc=  {color(round(total_tr_acc * 100,2))} | Epoch Duration: {time.strftime('%H:%M:%S', time.gmtime(epoch_end - epoch_start))}\")\n",
    "        elif verbose in [1]:\n",
    "            print(f\"\\033[K Epoch: {epoch + 1}/ {epochs}: Train Acc=  {color(round(total_tr_acc * 100,2))}\", end = '\\r')\n",
    "    training_end = time.time()\n",
    "    if verbose in [1, 2, 3]:\n",
    "        print(f\"\\nFinished training!  Training duration: {time.strftime('%H:%M:%S', time.gmtime(training_end - training_start))}\")\n",
    "    return train_history\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wVCJ3c1fTJqO"
   },
   "source": [
    "def evaluate(model, state, dataloader, loss_ratio , criterion, m, verbose = 1):\n",
    "    model.paste(state)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        ts_corrects = 0\n",
    "        test_inputs, test_labels = torch.tensor(dataloader.dataset.data_test).float().to(device), torch.tensor(dataloader.dataset.labels_test).to(device)\n",
    "        test_outputs, test_data =  model(test_inputs)\n",
    "        CSP_output_softmax = F.softmax(test_data['CSP output'], dim = -1)\n",
    "        CSP_loss = 0\n",
    "        BCElabels = output_format(test_labels, m)\n",
    "        for f in range(CSP_output_softmax.size(1)):\n",
    "            CSP_loss += nn.BCELoss()(CSP_output_softmax[:,f,...].float(), BCElabels)\n",
    "        CSP_loss = CSP_loss / CSP_output_softmax.size(1)  \n",
    "        test_loss = loss_ratio * CSP_loss + (1 - loss_ratio) * criterion(test_outputs, test_labels.type(torch.LongTensor).to(device))\n",
    "        predicted_labels = model.pred(test_inputs, test_labels).to(device)\n",
    "        ts_corrects += (predicted_labels == test_labels).sum().item()\n",
    "        ts_acc =  (predicted_labels == test_labels).sum().item()/(len(test_inputs))\n",
    "    if verbose > 0:\n",
    "        print((f\"\\033[K ► Test Loss=  {test_loss.item():.4f}  Test Acc= {color(round(ts_acc * 100, 2))}\"))\n",
    "    return test_loss, ts_acc"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI6WV7woMSLP"
   },
   "source": [
    "#Run Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqWpxmPUN8nF"
   },
   "source": [
    "Mode parameter: 'SD' for subject-dependent and 'SI' for subject-independent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hSbbiIcXOZR7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1466ebb0-1119-486f-e92a-f4230e707692"
   },
   "source": [
    "hists_acc = []\n",
    "hists_loss = []\n",
    "manual_seed(2045)\n",
    "\n",
    "for i in range(1,55):\n",
    "    subject = i\n",
    "    print('Subject:', i)\n",
    "    dataset = EEG_MI_dataset(data, labels, subject = subject, mode = 'SD')\n",
    "    ### Define Dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size = 5300, \n",
    "                            num_workers = workers, pin_memory = True,\n",
    "                            shuffle = True)\n",
    "    net_args = {\"kernLength\"                  : 32,\n",
    "                \"timepoints\"                  : 250,\n",
    "                \"wavelet_filters\"              : 4,\n",
    "                \"wavelet_kernel\"              : 64,\n",
    "                \"nn_layers\"                   : [4],\n",
    "                \"feature_reduction_network\"   : [16,8,4],\n",
    "                \"n_CSP_subspace_vectors\"      : 2,\n",
    "                \"nb_classes\"                  : 2\n",
    "    }\n",
    "\n",
    "    train_args = {\"dataloader\"      : dataloader,\n",
    "                \"epochs\"          : 20,\n",
    "                \"lr\"              : 0.01,\n",
    "                \"wavelet_lr\"      : 0.001,\n",
    "                \"loss_ratio\"      : 0.3,\n",
    "                \"criterion\"       : MVLoss,\n",
    "                \"verbose\"         : 0,\n",
    "                \"tensorboard\"     : False,\n",
    "                \"m\"               : net_args['n_CSP_subspace_vectors'],\n",
    "                'lambda1'         : 0.01,\n",
    "                'lambda2'         : 0.1\n",
    "    }\n",
    "\n",
    "    net = CCSP(**net_args).to(device)\n",
    "\n",
    "    history = train(model = net, **train_args)\n",
    "    ### Evaluate\n",
    "    best_model_state = history['model'][np.argmin(history['train_loss'])]\n",
    "    print('The Best Epoch:', np.argmin(history['train_loss']) + 1)\n",
    "    test_loss, ts_acc= evaluate(net,best_model_state, dataloader, m=net_args['n_CSP_subspace_vectors'], criterion= MVLoss,\n",
    "            loss_ratio = train_args[\"loss_ratio\"], verbose=1,)\n",
    "    hists_acc.append([test_loss, ts_acc])\n",
    "\n",
    "    best_model_state = history['model'][np.argmax(history['train_accuracy'])]\n",
    "    print('The Best Epoch based on accuracy:', np.argmax(history['train_accuracy']) + 1)\n",
    "    test_loss, ts_acc= evaluate(net,best_model_state, dataloader, m=net_args['n_CSP_subspace_vectors'], criterion= MVLoss,\n",
    "            loss_ratio = train_args[\"loss_ratio\"], verbose=1,)\n",
    "    hists_loss.append([test_loss, ts_acc])"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 13\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m### Define Dataloader\u001B[39;00m\n\u001B[1;32m     10\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5300\u001B[39m, \n\u001B[1;32m     11\u001B[0m                         num_workers \u001B[38;5;241m=\u001B[39m workers, pin_memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     12\u001B[0m                         shuffle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 13\u001B[0m net_args \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkernLength\u001B[39m\u001B[38;5;124m\"\u001B[39m                  : \u001B[38;5;241m32\u001B[39m,\n\u001B[1;32m     14\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimepoints\u001B[39m\u001B[38;5;124m\"\u001B[39m                  : \u001B[38;5;241m250\u001B[39m,\n\u001B[1;32m     15\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwavelet_filters\u001B[39m\u001B[38;5;124m\"\u001B[39m              : \u001B[38;5;241m4\u001B[39m,\n\u001B[1;32m     16\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwavelet_kernel\u001B[39m\u001B[38;5;124m\"\u001B[39m              : \u001B[38;5;241m64\u001B[39m,\n\u001B[1;32m     17\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnn_layers\u001B[39m\u001B[38;5;124m\"\u001B[39m                   : [\u001B[38;5;241m4\u001B[39m],\n\u001B[1;32m     18\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature_reduction_network\u001B[39m\u001B[38;5;124m\"\u001B[39m   : [\u001B[38;5;241m16\u001B[39m,\u001B[38;5;241m8\u001B[39m,\u001B[38;5;241m4\u001B[39m],\n\u001B[1;32m     19\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_CSP_subspace_vectors\u001B[39m\u001B[38;5;124m\"\u001B[39m      : \u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m     20\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnb_classes\u001B[39m\u001B[38;5;124m\"\u001B[39m                  : \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     21\u001B[0m }\n\u001B[1;32m     23\u001B[0m train_args \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataloader\u001B[39m\u001B[38;5;124m\"\u001B[39m      : dataloader,\n\u001B[1;32m     24\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m          : \u001B[38;5;241m20\u001B[39m,\n\u001B[1;32m     25\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m              : \u001B[38;5;241m0.01\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     33\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlambda2\u001B[39m\u001B[38;5;124m'\u001B[39m         : \u001B[38;5;241m0.1\u001B[39m\n\u001B[1;32m     34\u001B[0m }\n\u001B[1;32m     36\u001B[0m net \u001B[38;5;241m=\u001B[39m CCSP(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mnet_args)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "Cell \u001B[0;32mIn[13], line 13\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m### Define Dataloader\u001B[39;00m\n\u001B[1;32m     10\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5300\u001B[39m, \n\u001B[1;32m     11\u001B[0m                         num_workers \u001B[38;5;241m=\u001B[39m workers, pin_memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     12\u001B[0m                         shuffle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 13\u001B[0m net_args \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkernLength\u001B[39m\u001B[38;5;124m\"\u001B[39m                  : \u001B[38;5;241m32\u001B[39m,\n\u001B[1;32m     14\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimepoints\u001B[39m\u001B[38;5;124m\"\u001B[39m                  : \u001B[38;5;241m250\u001B[39m,\n\u001B[1;32m     15\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwavelet_filters\u001B[39m\u001B[38;5;124m\"\u001B[39m              : \u001B[38;5;241m4\u001B[39m,\n\u001B[1;32m     16\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwavelet_kernel\u001B[39m\u001B[38;5;124m\"\u001B[39m              : \u001B[38;5;241m64\u001B[39m,\n\u001B[1;32m     17\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnn_layers\u001B[39m\u001B[38;5;124m\"\u001B[39m                   : [\u001B[38;5;241m4\u001B[39m],\n\u001B[1;32m     18\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature_reduction_network\u001B[39m\u001B[38;5;124m\"\u001B[39m   : [\u001B[38;5;241m16\u001B[39m,\u001B[38;5;241m8\u001B[39m,\u001B[38;5;241m4\u001B[39m],\n\u001B[1;32m     19\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_CSP_subspace_vectors\u001B[39m\u001B[38;5;124m\"\u001B[39m      : \u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m     20\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnb_classes\u001B[39m\u001B[38;5;124m\"\u001B[39m                  : \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     21\u001B[0m }\n\u001B[1;32m     23\u001B[0m train_args \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataloader\u001B[39m\u001B[38;5;124m\"\u001B[39m      : dataloader,\n\u001B[1;32m     24\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m          : \u001B[38;5;241m20\u001B[39m,\n\u001B[1;32m     25\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m              : \u001B[38;5;241m0.01\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     33\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlambda2\u001B[39m\u001B[38;5;124m'\u001B[39m         : \u001B[38;5;241m0.1\u001B[39m\n\u001B[1;32m     34\u001B[0m }\n\u001B[1;32m     36\u001B[0m net \u001B[38;5;241m=\u001B[39m CCSP(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mnet_args)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
