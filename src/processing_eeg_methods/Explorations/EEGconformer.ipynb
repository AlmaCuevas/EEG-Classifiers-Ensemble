{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = 'conformer'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from braindecode.models import EEGConformer\n",
    "\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from processing_eeg_methods.data_utils import (\n",
    "    get_dataset_basic_info,\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from processing_eeg_methods.data_loaders import load_data_labels_based_on_dataset\n",
    "from processing_eeg_methods.share import datasets_basic_infos\n",
    "from collections import Counter\n",
    "\n",
    "dataset_name = \"braincommand\"  # Only two things I should be able to change\n",
    "\n",
    "dataset_info = get_dataset_basic_info(datasets_basic_infos, dataset_name)\n",
    "\n",
    "data_path = r\"C:\\Users\\rosit\\Documents\\workprojects\\bci_complete\\EEG-Classifiers-Ensemble\\Datasets\\braincommand_dataset\"\n",
    "subject_ids = range(1, 27)\n",
    "results_df = pd.DataFrame(columns=['subject_id', 'test_accuracy', 'f1_score_macro'])\n",
    "\n",
    "# Loop through each subject\n",
    "for subject_id in subject_ids:\n",
    "\n",
    "    epochs_calibration, X_calibration, y_calibration_original = load_data_labels_based_on_dataset(\n",
    "        dataset_info,\n",
    "        subject_id,\n",
    "        data_path,\n",
    "        game_mode=\"calibration\",\n",
    "    )\n",
    "\n",
    "    epochs_singleplayer, X_singleplayer, y_singleplayer_original = load_data_labels_based_on_dataset(\n",
    "        dataset_info,\n",
    "        subject_id,\n",
    "        data_path,\n",
    "        game_mode=\"singleplayer\",\n",
    "    )\n",
    "    y_calibration = [0] * len(y_calibration_original)\n",
    "    y_singleplayer = [1] * len(y_singleplayer_original)\n",
    "\n",
    "    X = np.concatenate((X_calibration, X_singleplayer), axis=0)\n",
    "    y = y_calibration + y_singleplayer\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Split the temp data into 50% test and 50% validation, resulting in 25% of the original data each\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    print(y_train)\n",
    "    num_classess = len(set(y_train))\n",
    "    print(f'There are {num_classess} unique classes in the dataset')\n",
    "\n",
    "    kernels, chans, samples = 1, dataset_info[\"#_channels\"], dataset_info[\"samples\"]\n",
    "\n",
    "    # y_train = y_train - 1\n",
    "    # y_val = y_val - 1\n",
    "    # y_test = y_test - 1\n",
    "\n",
    "    # X_train      = X_train.reshape(X_train.shape[0], chans, samples)\n",
    "    # X_val   = X_val.reshape(X_val.shape[0], chans, samples)\n",
    "    # X_test       = X_test.reshape(X_test.shape[0], chans, samples)\n",
    "\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Keep as integers\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    train_loader_size = len(train_loader)\n",
    "    print(f\"Number of batches in train_loader: {train_loader_size}\")\n",
    "\n",
    "    counts = Counter(y)\n",
    "\n",
    "    total_samples = sum(counts.values())\n",
    "\n",
    "    num_classes = len(counts)\n",
    "\n",
    "    class_weights = {class_label - 1: total_samples / (num_classes * count) for class_label, count in counts.items()}\n",
    "\n",
    "    print(class_weights)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = 'cpu'\n",
    "\n",
    "    n_classes = dataset_info[\"#_class\"]\n",
    "    classes = list(range(n_classes))\n",
    "\n",
    "    n_outputs = dataset_info[\"#_class\"]\n",
    "    n_chans = dataset_info[\"#_channels\"]\n",
    "    n_filters_time = 40\n",
    "    filter_time_length = 25\n",
    "    pool_time_length = 75\n",
    "    pool_time_stride = 15\n",
    "    drop_prob = 0.5\n",
    "    att_depth = 6\n",
    "    att_heads = 10\n",
    "    att_drop_prob = 0.5\n",
    "    final_fc_length = 640\n",
    "    return_features = False\n",
    "    n_times = dataset_info[\"samples\"]\n",
    "    chs_info = None\n",
    "    input_window_seconds = None\n",
    "    sfreq = dataset_info[\"sample_rate\"]\n",
    "    add_log_softmax = True\n",
    "\n",
    "    # Initialize the EEGConformer model\n",
    "    conformer = EEGConformer(\n",
    "        n_outputs=n_outputs,\n",
    "        n_chans=n_chans,\n",
    "        n_filters_time=n_filters_time,\n",
    "        filter_time_length=filter_time_length,\n",
    "        pool_time_length=pool_time_length,\n",
    "        pool_time_stride=pool_time_stride,\n",
    "        drop_prob=drop_prob,\n",
    "        att_depth=att_depth,\n",
    "        att_heads=att_heads,\n",
    "        att_drop_prob=att_drop_prob,\n",
    "        final_fc_length=final_fc_length,\n",
    "        return_features=return_features,\n",
    "        n_times=n_times,\n",
    "        chs_info=chs_info,\n",
    "        input_window_seconds=input_window_seconds,\n",
    "        sfreq=sfreq,\n",
    "        add_log_softmax=add_log_softmax\n",
    "    )\n",
    "    conformer = conformer.to(device)\n",
    "\n",
    "    print(conformer)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(conformer.parameters(), lr=0.0002, betas=[0.5, 0.999])\n",
    "\n",
    "    import torch\n",
    "\n",
    "    conformer.to(device)\n",
    "\n",
    "    # Create a dummy input with the correct shape and move it to the same device\n",
    "    dummy_input = torch.randn(kernels, chans, samples).to(device)  # Batch size\n",
    "\n",
    "    # Pass the dummy input through the model up to the transformer encoder\n",
    "    try:\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            x = torch.unsqueeze(dummy_input, dim=1)  # Add an extra dimension to match input shape\n",
    "            x = conformer.patch_embedding(x)  # Pass through Patch Embedding\n",
    "            x = conformer.transformer(x)  # Pass through Transformer Encoder\n",
    "\n",
    "            # Get the shape after the transformer and calculate the new `final_fc_length`\n",
    "            print(f\"Output shape after transformer: {x.shape}\")\n",
    "            final_fc_length_calculated = x.shape[1] * x.shape[2]\n",
    "            print(f\"Calculated `final_fc_length`: {final_fc_length_calculated}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during partial forward pass:\", str(e))\n",
    "\n",
    "    num_epochs = 250\n",
    "    ## TRAIN\n",
    "    for epoch in range(num_epochs):\n",
    "        conformer.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = conformer(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == y_batch).sum().item()\n",
    "            total_predictions += y_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "        ## Validation\n",
    "        conformer.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                outputs = conformer(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == y_batch).sum().item()\n",
    "                total_predictions += y_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    ## TEST\n",
    "    test_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            outputs = conformer(X_batch)\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "            correct_predictions += (predicted == y_batch).sum().item()\n",
    "            total_predictions += y_batch.size(0)\n",
    "\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1:.4f}\")\n",
    "\n",
    "    temp_df = pd.DataFrame({'subject_id': [subject_id], 'test_accuracy': [test_accuracy], 'f1_score_macro': [f1]})\n",
    "\n",
    "    # Concatenate the temporary dataframe with the results dataframe\n",
    "    results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "\n",
    "mean_accuracy = results_df['test_accuracy'].mean()\n",
    "std_accuracy = results_df['test_accuracy'].std()\n",
    "min_accuracy = results_df['test_accuracy'].min()\n",
    "max_accuracy = results_df['test_accuracy'].max()\n",
    "mean_f1 = results_df['f1_score_macro'].mean()\n",
    "std_f1 = results_df['f1_score_macro'].std()\n",
    "min_f1 = results_df['f1_score_macro'].min()\n",
    "max_f1 = results_df['f1_score_macro'].max()\n",
    "\n",
    "# Report the results\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"{'Overall Performance':^40}\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Mean Test Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy Standard Deviation: {std_accuracy:.4f}\")\n",
    "print(f\"Minimum Test Accuracy: {min_accuracy:.4f}\")\n",
    "print(f\"Maximum Test Accuracy: {max_accuracy:.4f}\")\n",
    "print(f\"Mean F1 Score (Macro): {mean_f1:.4f}\")\n",
    "print(f\"F1 Score (Macro) Standard Deviation: {std_f1:.4f}\")\n",
    "print(f\"Minimum F1 Score (Macro): {min_f1:.4f}\")\n",
    "print(f\"Maximum F1 Score (Macro): {max_f1:.4f}\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(r\"C:\\Users\\rosit\\Documents\\workprojects\\bci_complete\\EEG-Classifiers-Ensemble\\Results\\braincommand\\FINAL\\conformer_subject_results_cleaned.csv\", index=False)\n",
    "print(\"\\nResults saved to conformer_subject_results.csv\")\n",
    "\n",
    "# Set the Seaborn palette\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Create a boxplot for test accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=results_df['test_accuracy'])\n",
    "plt.title('Boxplot of Test Accuracies per Subject')\n",
    "plt.ylabel('Test Accuracy')\n",
    "\n",
    "# Create a boxplot for F1 scores\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=results_df['f1_score_macro'])\n",
    "plt.title('Boxplot of Macro F1 Scores per Subject')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f5448a60bdd46f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-11T16:36:59.920880300Z",
     "start_time": "2025-04-11T16:36:59.680367900Z"
    }
   },
   "id": "502d67ccb290fa5e",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3da3f6017129afac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
