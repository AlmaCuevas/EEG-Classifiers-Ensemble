{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667cbb0e-b42e-4ea6-a195-b296702614ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:18:22.933969Z",
     "start_time": "2024-09-21T16:18:22.914209Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'conformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c504eba-a256-4ddb-bc17-55b59d27a609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:18:31.234516300Z",
     "start_time": "2024-09-21T16:18:23.077195Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rosit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from braindecode.models import EEGConformer\n",
    "\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae83aafa-2a37-42f7-9ace-d937ca00c107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:18:31.277361400Z",
     "start_time": "2024-09-21T16:18:31.235514600Z"
    }
   },
   "outputs": [],
   "source": [
    "from processing_eeg_methods.data_utils import (\n",
    "    convert_into_independent_channels,\n",
    "    get_dataset_basic_info,\n",
    "    get_input_data_path,\n",
    "    standard_saving_path,\n",
    "    write_model_info,\n",
    ")\n",
    "from processing_eeg_methods.data_loaders import load_data_labels_based_on_dataset\n",
    "from processing_eeg_methods.share import datasets_basic_infos\n",
    "\n",
    "subject_id = 8  # Only two things I should be able to change\n",
    "dataset_name = \"braincommand\"  # Only two things I should be able to change\n",
    "\n",
    "get_dataset_basic_info(datasets_basic_infos, dataset_name)\n",
    "dataset_info: dict = datasets_basic_infos[dataset_name]\n",
    "\n",
    "data_path = r\"C:\\Users\\rosit\\Documents\\workprojects\\bci_complete\\EEG-Classifiers-Ensemble\\Datasets\\braincommand_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0 is 85\n",
      "label 1 is 102\n",
      "label 2 is 65\n",
      "label 3 is 23\n",
      "Not setting metadata\n",
      "275 matching events found\n",
      "Setting baseline interval to [0.0, 1.296] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "epochs_calibration, X_calibration, y_calibration_original = load_data_labels_based_on_dataset(\n",
    "    dataset_info,\n",
    "    subject_id,\n",
    "    data_path,\n",
    "    game_mode=\"calibration3\",\n",
    ")\n",
    "\n",
    "epochs_singleplayer, X_singleplayer, y_singleplayer_original = load_data_labels_based_on_dataset(\n",
    "    dataset_info,\n",
    "    subject_id,\n",
    "    data_path,\n",
    "    game_mode=\"singleplayer\",\n",
    ")\n",
    "y_calibration = [0] * len(y_calibration_original)\n",
    "y_singleplayer = [1] * len(y_singleplayer_original)\n",
    "\n",
    "X = np.concatenate((X_calibration, X_singleplayer), axis=0)\n",
    "y = y_calibration + y_singleplayer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-20T20:46:44.317751Z",
     "start_time": "2024-09-20T20:46:39.047144300Z"
    }
   },
   "id": "2295ef4877e545bd",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from C:\\Users\\rosit\\Documents\\MATLAB\\clean EEG BrainCommand\\datasets_edited\\8_calibration3_cleaned.set...\n",
      "Not setting metadata\n",
      "166 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "filepath = r\"C:\\Users\\rosit\\Documents\\MATLAB\\clean EEG BrainCommand\\datasets_edited/8_calibration3_cleaned.set\"\n",
    "epochs = mne.io.read_epochs_eeglab(filepath, verbose=True)\n",
    "X = epochs.get_data()\n",
    "y = epochs.events[:, 2].astype(np.int64)\n",
    "y = y - 1\n",
    "dataset_info[\"#_channels\"] = 6"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:42.942738300Z",
     "start_time": "2024-09-21T16:27:42.895555900Z"
    }
   },
   "id": "34e833b36917c8bc",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84caa094-f91e-45cd-bb84-e4b502230981",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:48.919680600Z",
     "start_time": "2024-09-21T16:27:48.833880700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 3 2 1 0 0 3 2 1 1 1 3 3 0 1 1 3 2 1 1 1 0 2 0 3 3 3 3 1 0 3 1 0 3 3\n",
      " 0 2 1 2 3 3 3 3 2 2 2 2 3 3 0 1 2 1 1 0 0 2 0 2 2 0 0 3 1 2 0 0 0 1 1 1 0\n",
      " 1 1 3 3 1 0 1 1 3]\n",
      "There are 4 unique classes in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 50% training and 50% temp (which will later be split into test and validation)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Split the temp data into 50% test and 50% validation, resulting in 25% of the original data each\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(y_train)\n",
    "num_classess = len(set(y_train))\n",
    "print(f'There are {num_classess} unique classes in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "179bcbab-3c7c-4010-86ef-3749fd1120ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:49.126868Z",
     "start_time": "2024-09-21T16:27:49.039116400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 2, 2, 3, 2, 1, 0, 0, 3, 2, 1, 1, 1, 3, 3, 0, 1, 1, 3, 2, 1, 1,\n       1, 0, 2, 0, 3, 3, 3, 3, 1, 0, 3, 1, 0, 3, 3, 0, 2, 1, 2, 3, 3, 3,\n       3, 2, 2, 2, 2, 3, 3, 0, 1, 2, 1, 1, 0, 0, 2, 0, 2, 2, 0, 0, 3, 1,\n       2, 0, 0, 0, 1, 1, 1, 0, 1, 1, 3, 3, 1, 0, 1, 1, 3], dtype=int64)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c53d556-affe-4b90-a202-237624ae2400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:49.247139500Z",
     "start_time": "2024-09-21T16:27:49.136405100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (83, 7, 325)\n",
      "83 train samples\n",
      "41 test samples\n"
     ]
    }
   ],
   "source": [
    "kernels, chans, samples = 1, dataset_info[\"#_channels\"], dataset_info[\"samples\"]\n",
    "\n",
    "# y_train = y_train - 1\n",
    "# y_val = y_val - 1\n",
    "# y_test = y_test - 1\n",
    "\n",
    "# X_train      = X_train.reshape(X_train.shape[0], chans, samples)\n",
    "# X_val   = X_val.reshape(X_val.shape[0], chans, samples)\n",
    "# X_test       = X_test.reshape(X_test.shape[0], chans, samples)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Keep as integers\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "303908e7-1ac7-4491-8ac4-a83aa51c49bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:49.388075100Z",
     "start_time": "2024-09-21T16:27:49.285235900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_loader: 2\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of batches in train_loader\n",
    "train_loader_size = len(train_loader)\n",
    "print(f\"Number of batches in train_loader: {train_loader_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84bd7fd7-e279-4068-9e75-5aa6321f1870",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:49.482795600Z",
     "start_time": "2024-09-21T16:27:49.411640400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: 0.9651162790697675, 0: 0.9431818181818182, 1: 1.0921052631578947, 2: 1.0121951219512195}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(y)\n",
    "\n",
    "total_samples = sum(counts.values())\n",
    "\n",
    "num_classes = len(counts)\n",
    "\n",
    "class_weights = {class_label-1: total_samples / (num_classes * count) for class_label, count in counts.items()}\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "094b6a7c-ec20-44c0-a3e2-99cee2a60f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:49.669085900Z",
     "start_time": "2024-09-21T16:27:49.594135Z"
    }
   },
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1664cc4c-f8e6-40d3-9389-0a9a8732b5c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:50.678225500Z",
     "start_time": "2024-09-21T16:27:49.690529400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rosit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                            Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "================================================================================================================================================================\n",
      "EEGConformer (EEGConformer)                                  [1, 6, 325]               [1, 4]                    --                        --\n",
      "├─_PatchEmbedding (patch_embedding): 1-1                     [1, 1, 6, 325]            [1, 16, 40]               --                        --\n",
      "│    └─Sequential (shallownet): 2-1                          [1, 1, 6, 325]            [1, 40, 1, 16]            --                        --\n",
      "│    │    └─Conv2d (0): 3-1                                  [1, 1, 6, 325]            [1, 40, 6, 301]           1,040                     [1, 25]\n",
      "│    │    └─Conv2d (1): 3-2                                  [1, 40, 6, 301]           [1, 40, 1, 301]           9,640                     [6, 1]\n",
      "│    │    └─BatchNorm2d (2): 3-3                             [1, 40, 1, 301]           [1, 40, 1, 301]           80                        --\n",
      "│    │    └─ELU (3): 3-4                                     [1, 40, 1, 301]           [1, 40, 1, 301]           --                        --\n",
      "│    │    └─AvgPool2d (4): 3-5                               [1, 40, 1, 301]           [1, 40, 1, 16]            --                        [1, 75]\n",
      "│    │    └─Dropout (5): 3-6                                 [1, 40, 1, 16]            [1, 40, 1, 16]            --                        --\n",
      "│    └─Sequential (projection): 2-2                          [1, 40, 1, 16]            [1, 16, 40]               --                        --\n",
      "│    │    └─Conv2d (0): 3-7                                  [1, 40, 1, 16]            [1, 40, 1, 16]            1,640                     [1, 1]\n",
      "│    │    └─Rearrange (1): 3-8                               [1, 40, 1, 16]            [1, 16, 40]               --                        --\n",
      "├─_TransformerEncoder (transformer): 1-2                     [1, 16, 40]               [1, 16, 40]               --                        --\n",
      "│    └─_TransformerEncoderBlock (0): 2-3                     [1, 16, 40]               [1, 16, 40]               --                        --\n",
      "│    │    └─_ResidualAdd (0): 3-9                            [1, 16, 40]               [1, 16, 40]               6,640                     --\n",
      "│    │    └─_ResidualAdd (1): 3-10                           [1, 16, 40]               [1, 16, 40]               13,080                    --\n",
      "│    └─_TransformerEncoderBlock (1): 2-4                     [1, 16, 40]               [1, 16, 40]               --                        --\n",
      "│    │    └─_ResidualAdd (0): 3-11                           [1, 16, 40]               [1, 16, 40]               6,640                     --\n",
      "│    │    └─_ResidualAdd (1): 3-12                           [1, 16, 40]               [1, 16, 40]               13,080                    --\n",
      "│    └─_TransformerEncoderBlock (2): 2-5                     [1, 16, 40]               [1, 16, 40]               --                        --\n",
      "│    │    └─_ResidualAdd (0): 3-13                           [1, 16, 40]               [1, 16, 40]               6,640                     --\n",
      "│    │    └─_ResidualAdd (1): 3-14                           [1, 16, 40]               [1, 16, 40]               13,080                    --\n",
      "│    └─_TransformerEncoderBlock (3): 2-6                     [1, 16, 40]               [1, 16, 40]               --                        --\n",
      "│    │    └─_ResidualAdd (0): 3-15                           [1, 16, 40]               [1, 16, 40]               6,640                     --\n",
      "│    │    └─_ResidualAdd (1): 3-16                           [1, 16, 40]               [1, 16, 40]               13,080                    --\n",
      "│    └─_TransformerEncoderBlock (4): 2-7                     [1, 16, 40]               [1, 16, 40]               --                        --\n",
      "│    │    └─_ResidualAdd (0): 3-17                           [1, 16, 40]               [1, 16, 40]               6,640                     --\n",
      "│    │    └─_ResidualAdd (1): 3-18                           [1, 16, 40]               [1, 16, 40]               13,080                    --\n",
      "│    └─_TransformerEncoderBlock (5): 2-8                     [1, 16, 40]               [1, 16, 40]               --                        --\n",
      "│    │    └─_ResidualAdd (0): 3-19                           [1, 16, 40]               [1, 16, 40]               6,640                     --\n",
      "│    │    └─_ResidualAdd (1): 3-20                           [1, 16, 40]               [1, 16, 40]               13,080                    --\n",
      "├─_FullyConnected (fc): 1-3                                  [1, 16, 40]               [1, 32]                   --                        --\n",
      "│    └─Sequential (fc): 2-9                                  [1, 640]                  [1, 32]                   --                        --\n",
      "│    │    └─Linear (0): 3-21                                 [1, 640]                  [1, 256]                  164,096                   --\n",
      "│    │    └─ELU (1): 3-22                                    [1, 256]                  [1, 256]                  --                        --\n",
      "│    │    └─Dropout (2): 3-23                                [1, 256]                  [1, 256]                  --                        --\n",
      "│    │    └─Linear (3): 3-24                                 [1, 256]                  [1, 32]                   8,224                     --\n",
      "│    │    └─ELU (4): 3-25                                    [1, 32]                   [1, 32]                   --                        --\n",
      "│    │    └─Dropout (5): 3-26                                [1, 32]                   [1, 32]                   --                        --\n",
      "├─_FinalLayer (final_layer): 1-4                             [1, 32]                   [1, 4]                    --                        --\n",
      "│    └─Sequential (final_layer): 2-10                        [1, 32]                   [1, 4]                    --                        --\n",
      "│    │    └─Linear (0): 3-27                                 [1, 32]                   [1, 4]                    132                       --\n",
      "│    │    └─LogSoftmax (classification): 3-28                [1, 4]                    [1, 4]                    --                        --\n",
      "================================================================================================================================================================\n",
      "Total params: 303,172\n",
      "Trainable params: 303,172\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 5.10\n",
      "================================================================================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.12\n",
      "Params size (MB): 1.21\n",
      "Estimated Total Size (MB): 2.34\n",
      "================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "n_classes = dataset_info[\"#_class\"]\n",
    "classes = list(range(n_classes))\n",
    "\n",
    "n_outputs = dataset_info[\"#_class\"]\n",
    "n_chans = dataset_info[\"#_channels\"]\n",
    "n_filters_time = 40\n",
    "filter_time_length = 25\n",
    "pool_time_length = 75\n",
    "pool_time_stride = 15\n",
    "drop_prob = 0.5\n",
    "att_depth = 6\n",
    "att_heads = 10\n",
    "att_drop_prob = 0.5\n",
    "final_fc_length = 640\n",
    "return_features = False\n",
    "n_times = dataset_info[\"samples\"]\n",
    "chs_info = None\n",
    "input_window_seconds = None\n",
    "sfreq = dataset_info[\"sample_rate\"]\n",
    "add_log_softmax = True\n",
    "\n",
    "# Initialize the EEGConformer model\n",
    "conformer = EEGConformer(\n",
    "    n_outputs=n_outputs,\n",
    "    n_chans=n_chans,\n",
    "    n_filters_time=n_filters_time,\n",
    "    filter_time_length=filter_time_length,\n",
    "    pool_time_length=pool_time_length,\n",
    "    pool_time_stride=pool_time_stride,\n",
    "    drop_prob=drop_prob,\n",
    "    att_depth=att_depth,\n",
    "    att_heads=att_heads,\n",
    "    att_drop_prob=att_drop_prob,\n",
    "    final_fc_length=final_fc_length,\n",
    "    return_features=return_features,\n",
    "    n_times=n_times,\n",
    "    chs_info=chs_info,\n",
    "    input_window_seconds=input_window_seconds,\n",
    "    sfreq=sfreq,\n",
    "    add_log_softmax=add_log_softmax\n",
    ")\n",
    "conformer = conformer.to(device)\n",
    "\n",
    "print(conformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f5f63-82a0-4fe3-94fe-d48aac560d54",
   "metadata": {},
   "source": [
    "Do this to find the right final_fc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fba38c99-5812-4e07-8566-5b50d2abd7ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:50.743967500Z",
     "start_time": "2024-09-21T16:27:50.723404500Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(conformer.parameters(), lr =  0.0002,betas  = [0.5, 0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e52920c-c26b-4d99-8636-7fb75e0c4b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:50.800644900Z",
     "start_time": "2024-09-21T16:27:50.740951900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after transformer: torch.Size([1, 16, 40])\n",
      "Calculated `final_fc_length`: 640\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "conformer.to(device)\n",
    "\n",
    "# Create a dummy input with the correct shape and move it to the same device\n",
    "dummy_input = torch.randn(kernels, chans, samples).to(device)  # Batch size\n",
    "\n",
    "# Pass the dummy input through the model up to the transformer encoder\n",
    "try:\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        x = torch.unsqueeze(dummy_input, dim=1)  # Add an extra dimension to match input shape\n",
    "        x = conformer.patch_embedding(x)  # Pass through Patch Embedding\n",
    "        x = conformer.transformer(x)  # Pass through Transformer Encoder\n",
    "        \n",
    "        # Get the shape after the transformer and calculate the new `final_fc_length`\n",
    "        print(f\"Output shape after transformer: {x.shape}\")\n",
    "        final_fc_length_calculated = x.shape[1] * x.shape[2]\n",
    "        print(f\"Calculated `final_fc_length`: {final_fc_length_calculated}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during partial forward pass:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40d5e274-df80-4d85-a648-9535b0b270ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T16:27:51.120941400Z",
     "start_time": "2024-09-21T16:27:50.804664300Z"
    }
   },
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": "Shape mismatch, 2 != 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mEinopsError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 15\u001B[0m\n\u001B[0;32m     11\u001B[0m X_batch, y_batch \u001B[38;5;241m=\u001B[39m X_batch\u001B[38;5;241m.\u001B[39mto(device), y_batch\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     13\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 15\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mconformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, y_batch)\n\u001B[0;32m     18\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\braindecode\\models\\eegconformer.py:165\u001B[0m, in \u001B[0;36mEEGConformer.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    164\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39munsqueeze(x, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# add one extra dimension\u001B[39;00m\n\u001B[1;32m--> 165\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatch_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(x)\n\u001B[0;32m    167\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(x)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\braindecode\\models\\eegconformer.py:245\u001B[0m, in \u001B[0;36m_PatchEmbedding.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    244\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshallownet(x)\n\u001B[1;32m--> 245\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprojection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    246\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\einops\\layers\\torch.py:15\u001B[0m, in \u001B[0;36mRearrange.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m     14\u001B[0m     recipe \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_multirecipe[\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mndim]\n\u001B[1;32m---> 15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mapply_for_scriptable_torch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrecipe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrearrange\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes_dims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_axes_lengths\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\einops\\_torch_specific.py:87\u001B[0m, in \u001B[0;36mapply_for_scriptable_torch\u001B[1;34m(recipe, tensor, reduction_type, axes_dims)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_for_scriptable_torch\u001B[39m(\n\u001B[0;32m     77\u001B[0m     recipe: TransformRecipe, tensor: torch\u001B[38;5;241m.\u001B[39mTensor, reduction_type: \u001B[38;5;28mstr\u001B[39m, axes_dims: List[Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mint\u001B[39m]]\n\u001B[0;32m     78\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m     79\u001B[0m     backend \u001B[38;5;241m=\u001B[39m TorchJitBackend\n\u001B[0;32m     80\u001B[0m     (\n\u001B[0;32m     81\u001B[0m         init_shapes,\n\u001B[0;32m     82\u001B[0m         axes_reordering,\n\u001B[0;32m     83\u001B[0m         reduced_axes,\n\u001B[0;32m     84\u001B[0m         added_axes,\n\u001B[0;32m     85\u001B[0m         final_shapes,\n\u001B[0;32m     86\u001B[0m         n_axes_w_added,\n\u001B[1;32m---> 87\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[43m_reconstruct_from_shape_uncached\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecipe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes_dims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxes_dims\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m init_shapes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     89\u001B[0m         tensor \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mreshape(tensor, init_shapes)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\einops\\einops.py:183\u001B[0m, in \u001B[0;36m_reconstruct_from_shape_uncached\u001B[1;34m(self, shape, axes_dims)\u001B[0m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(unknown_axes) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(length, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(known_product, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m length \u001B[38;5;241m!=\u001B[39m known_product:\n\u001B[1;32m--> 183\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m EinopsError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShape mismatch, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlength\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m != \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mknown_product\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;66;03m# assert len(unknown_axes) == 1, 'this is enforced when recipe is created, so commented out'\u001B[39;00m\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(length, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(known_product, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m length \u001B[38;5;241m%\u001B[39m known_product \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[1;31mEinopsError\u001B[0m: Shape mismatch, 2 != 1"
     ]
    }
   ],
   "source": [
    "num_epochs = 250\n",
    "## TRAIN\n",
    "for epoch in range(num_epochs):\n",
    "    conformer.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = conformer(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == y_batch).sum().item()\n",
    "        total_predictions += y_batch.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "## Validation\n",
    "    conformer.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            outputs = conformer(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == y_batch).sum().item()\n",
    "            total_predictions += y_batch.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568117a-3c9d-48b1-a90b-8ed2df89fe96",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-21T16:27:51.118426800Z"
    }
   },
   "outputs": [],
   "source": [
    "## TEST\n",
    "test_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        outputs = conformer(X_batch)\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "        correct_predictions += (predicted == y_batch).sum().item()\n",
    "        total_predictions += y_batch.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
